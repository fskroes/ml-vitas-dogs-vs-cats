{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search is a model hyperparameter optimization technique.\n",
    "\n",
    "When constructing this class you must provide a dictionary of hyperparameters to evaluate in the param_grid argument. This is a map of the model parameter name and an array of values to try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the grid search will only use one thread. By setting the n_jobs argument in the GridSearchCV constructor to -1, the process will use all cores on your machine. Depending on your Keras backend, this may interfere with the main neural network training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GridSearchCV process will then construct and evaluate one model for each combination of parameters. Cross validation is used to evaluate each individual model and the default of 3-fold cross validation is used, although this can be overridden by specifying the cv argument to the GridSearchCV constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value = 42\n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import SGD, Adamax\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img, ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preperation\n",
    "\n",
    "Make a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filenames = os.listdir(\"input/data/train\")\n",
    "categories = []\n",
    "file_with_path = []\n",
    "for filename in filenames:\n",
    "    category = filename.split('.')[0]\n",
    "    if category == 'dog':\n",
    "        categories.append(1)\n",
    "    else:\n",
    "        categories.append(0)\n",
    "    filename = 'input/data/train/'+filename\n",
    "    file_with_path.append(filename)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'filename': file_with_path,\n",
    "    'category': categories\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "count                             25000\n",
       "unique                            25000\n",
       "top       input/data/train/cat.9621.jpg\n",
       "freq                                  1\n",
       "Name: filename, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# See how DF looks like\n",
    "df.head()\n",
    "df.filename.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for creating the model to use by Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 128\n",
    "IMAGE_HEIGHT = 128\n",
    "IMAGE_SIZE = (IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "IMAGE_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data split into train and validation df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "20668it [00:55, 370.99it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ba2a750af7a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimg_pixel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMAGE_WIDTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGE_HEIGHT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mimg_pixel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.3/lib/python3.8/site-packages/tensorflow/python/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msupported\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m   \"\"\"\n\u001b[0;32m--> 300\u001b[0;31m   return image.load_img(path, grayscale=grayscale, color_mode=color_mode,\n\u001b[0m\u001b[1;32m    301\u001b[0m                         target_size=target_size, interpolation=interpolation)\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.3/lib/python3.8/site-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \", \".join(_PIL_INTERPOLATION_METHODS.keys())))\n\u001b[1;32m    137\u001b[0m                 \u001b[0mresample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_PIL_INTERPOLATION_METHODS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth_height_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.3/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   1908\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1910\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1912\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreducing_gap\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresample\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mNEAREST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.3/lib/python3.8/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m  \u001b[0;31m# initialize to unknown error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.3/lib/python3.8/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload_prepare\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;31m# create image memory if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         \u001b[0;31m# create palette (optional)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"P\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "img_pixel = []\n",
    "for i, img_id in tqdm(enumerate(df['filename'].values)):\n",
    "    img = load_img(img_id, target_size=(IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "    img = img_to_array(img)\n",
    "    img_pixel.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_pixel=np.array([img_to_array(load_img(img, target_size=IMAGE_SIZE)) for img in df['filename'].values.tolist() ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label encoding dog and cat for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_label=df.category\n",
    "img_label=pd.get_dummies(df.category)\n",
    "img_label.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final X, Y matrix for deep learning prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(25000, 128, 128, 3)\n(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "# X=np.array(img_pixel[:25000]) # for testing purpose, put shape to only use 1000 images\n",
    "# y=img_label.values # Same here. Array's needs to be a match\n",
    "# print(X.shape)\n",
    "# print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "source": [
    "# Creating callbacks (things to help our model)\n",
    "We've got a model ready to go but before we train it we'll make some callbacks\n",
    "\n",
    "Callbacks are helper functions a model can use during training to do things such as save a models progres, check a models progress or stop training early if a model stops improving."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Tensorboard Callback\n",
    "\n",
    "TensorBoard helps provide a visual way to monitor the progress of your model during and after training.\n",
    "\n",
    "It can be used directly in a notebook to track the performance measures of a model such as loss and accuracy.\n",
    "\n",
    "To set up a TensorBoard callback and view TensorBoard in a notbook, we need to do three things:\n",
    "\n",
    "1. Load the TensorBoard notebook extension.\n",
    "\n",
    "2. Create a TensorBoard callback which is able to save logs to a directory and pass it to our model's fit() function.\n",
    "\n",
    "3. Visualize the our models trainigs logs using %tensorboard magic function (we'll do this later on)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "# Create a function to build a TensorBoard callback\n",
    "def create_tensorboard_callback():\n",
    "    # Create a log directly for storing TensorBoard logs\n",
    "    logdir = os.path.join('logs', datetime.datetime.now().strftime('%d%m%Y-%H%M%S'))\n",
    "\n",
    "    return tf.keras.callbacks.TensorBoard(logdir)"
   ]
  },
  {
   "source": [
    "## Save checkpoints during training\n",
    "\n",
    "You can use a trained model without having to retrain it, or pick-up training where you left off in case the training process was interrupted. The tf.keras.callbacks.ModelCheckpoint callback allows you to continually save the model both during and at the end of training.\n",
    "\n",
    "Saves every 5 epoch a checkpoint"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_checkpoint(model):\n",
    "    checkpoint_path = \"training_checkpoint/cp-{epoch:04d}.ckpt\"\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "    # Create a callback that saves the model's weights\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1, save_freq=5)\n",
    "\n",
    "    # Save the weights using the `checkpoint_path` format\n",
    "    # model.save_weights(checkpoint_path.format(epoch=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperating the grid seard per search cat.\n",
    "\n",
    "First, optimizer\n",
    "\n",
    "Second, batch and epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "optimizer   = ['RMSprop', 'Adadelta', 'Adamax']\n",
    "\n",
    "def create_model(optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, epochs=50, batch_size=16, verbose=1)\n",
    "\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "\n",
    "create_checkpoint(model)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=3)\n",
    "grid_result = grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn rate and momentum search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "learn_rate  = [0.1, 0.01, 0.001]\n",
    "momentum    = [0.0, 0.4, 0.8]\n",
    "\n",
    "def create_model(learn_rate=0.01, momentum=0):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    optimizer = Adamax(learning_rate=learn_rate)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "param_grid = dict(learn_rate=learn_rate, momentum=momentum)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=3)\n",
    "grid_result = grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch size and Epoch GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LossHistory(tf.keras.callbacks.Callback):\n",
    "#     def on_train_begin(self, logs={}):\n",
    "#         self.losses = []\n",
    "\n",
    "#     def on_epoch_end(self, batch, logs={}):\n",
    "#         with open('somefile.txt', 'a') as f:\n",
    "#             stats = []\n",
    "#             stats.append(str(batch))\n",
    "#             stats.append('Optimizer,' + self.model.optimizer.__class__.__name__)\n",
    "#             stats.append('Batch_size,' + str(self.params['batch_size']))\n",
    "#             stats.append('accuracy,'+str(logs.get('accuracy')))\n",
    "#             stats.append('val_loss,'+str(logs.get('val_loss')))\n",
    "#             f.write(','.join(stats)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "  1/417 [..............................] - ETA: 0s - loss: 1.4596 - accuracy: 0.4062WARNING:tensorflow:From /Users/fskroes/.pyenv/versions/3.8.3/lib/python3.8/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "417/417 [==============================] - 169s 405ms/step - loss: 0.8067 - accuracy: 0.6608\n",
      "Epoch 2/30\n",
      "417/417 [==============================] - 175s 419ms/step - loss: 0.5138 - accuracy: 0.7656\n",
      "Epoch 3/30\n",
      "417/417 [==============================] - 172s 414ms/step - loss: 0.4192 - accuracy: 0.8110\n",
      "Epoch 4/30\n",
      "417/417 [==============================] - 170s 408ms/step - loss: 0.3342 - accuracy: 0.8555\n",
      "Epoch 5/30\n",
      "417/417 [==============================] - 170s 408ms/step - loss: 0.2655 - accuracy: 0.8914\n",
      "Epoch 6/30\n",
      "417/417 [==============================] - 170s 407ms/step - loss: 0.1822 - accuracy: 0.9278\n",
      "Epoch 7/30\n",
      "417/417 [==============================] - 169s 406ms/step - loss: 0.1271 - accuracy: 0.9503\n",
      "Epoch 8/30\n",
      "417/417 [==============================] - 170s 408ms/step - loss: 0.1067 - accuracy: 0.9591\n",
      "Epoch 9/30\n",
      "417/417 [==============================] - 169s 406ms/step - loss: 0.0778 - accuracy: 0.9716\n",
      "Epoch 10/30\n",
      "417/417 [==============================] - 174s 416ms/step - loss: 0.0700 - accuracy: 0.9743\n",
      "Epoch 11/30\n",
      "417/417 [==============================] - 173s 415ms/step - loss: 0.0517 - accuracy: 0.9819\n",
      "Epoch 12/30\n",
      "417/417 [==============================] - 167s 401ms/step - loss: 0.0475 - accuracy: 0.9819\n",
      "Epoch 13/30\n",
      "417/417 [==============================] - 166s 399ms/step - loss: 0.0372 - accuracy: 0.9881\n",
      "Epoch 14/30\n",
      "417/417 [==============================] - 168s 404ms/step - loss: 0.0411 - accuracy: 0.9840\n",
      "Epoch 15/30\n",
      "417/417 [==============================] - 166s 399ms/step - loss: 0.0333 - accuracy: 0.9896\n",
      "Epoch 16/30\n",
      "417/417 [==============================] - 167s 400ms/step - loss: 0.0277 - accuracy: 0.9908\n",
      "Epoch 17/30\n",
      "417/417 [==============================] - 168s 403ms/step - loss: 0.0328 - accuracy: 0.9877\n",
      "Epoch 18/30\n",
      "417/417 [==============================] - 167s 401ms/step - loss: 0.0243 - accuracy: 0.9914\n",
      "Epoch 19/30\n",
      "417/417 [==============================] - 166s 399ms/step - loss: 0.0294 - accuracy: 0.9896\n",
      "Epoch 20/30\n",
      "417/417 [==============================] - 168s 402ms/step - loss: 0.0207 - accuracy: 0.9927\n",
      "Epoch 21/30\n",
      "417/417 [==============================] - 168s 403ms/step - loss: 0.0259 - accuracy: 0.9912\n",
      "Epoch 22/30\n",
      "417/417 [==============================] - 168s 403ms/step - loss: 0.0195 - accuracy: 0.9932\n",
      "Epoch 23/30\n",
      "417/417 [==============================] - 169s 404ms/step - loss: 0.0236 - accuracy: 0.9923\n",
      "Epoch 24/30\n",
      "417/417 [==============================] - 166s 399ms/step - loss: 0.0299 - accuracy: 0.9883\n",
      "Epoch 25/30\n",
      "417/417 [==============================] - 168s 404ms/step - loss: 0.0167 - accuracy: 0.9949\n",
      "Epoch 26/30\n",
      "417/417 [==============================] - 167s 401ms/step - loss: 0.0164 - accuracy: 0.9946\n",
      "Epoch 27/30\n",
      "417/417 [==============================] - 167s 401ms/step - loss: 0.0178 - accuracy: 0.9944\n",
      "Epoch 28/30\n",
      "417/417 [==============================] - 168s 402ms/step - loss: 0.0163 - accuracy: 0.9944\n",
      "Epoch 29/30\n",
      "417/417 [==============================] - 168s 402ms/step - loss: 0.0172 - accuracy: 0.9944\n",
      "Epoch 30/30\n",
      "417/417 [==============================] - 167s 401ms/step - loss: 0.0164 - accuracy: 0.9941\n",
      "209/209 [==============================] - 17s 79ms/step - loss: 0.8457 - accuracy: 0.8130\n",
      "Epoch 1/30\n",
      "417/417 [==============================] - 168s 404ms/step - loss: 0.8108 - accuracy: 0.6720\n",
      "Epoch 2/30\n",
      "417/417 [==============================] - 171s 410ms/step - loss: 0.5273 - accuracy: 0.7615\n",
      "Epoch 3/30\n",
      "417/417 [==============================] - 172s 411ms/step - loss: 0.4142 - accuracy: 0.8150\n",
      "Epoch 4/30\n",
      "417/417 [==============================] - 172s 413ms/step - loss: 0.3276 - accuracy: 0.8606\n",
      "Epoch 5/30\n",
      "417/417 [==============================] - 165s 395ms/step - loss: 0.2401 - accuracy: 0.9024\n",
      "Epoch 6/30\n",
      "417/417 [==============================] - 163s 392ms/step - loss: 0.1666 - accuracy: 0.9365\n",
      "Epoch 7/30\n",
      "417/417 [==============================] - 166s 398ms/step - loss: 0.1099 - accuracy: 0.9587\n",
      "Epoch 8/30\n",
      "417/417 [==============================] - 164s 393ms/step - loss: 0.0837 - accuracy: 0.9699\n",
      "Epoch 9/30\n",
      "417/417 [==============================] - 164s 394ms/step - loss: 0.0699 - accuracy: 0.9746\n",
      "Epoch 10/30\n",
      "417/417 [==============================] - 164s 394ms/step - loss: 0.0556 - accuracy: 0.9804\n",
      "Epoch 11/30\n",
      "417/417 [==============================] - 165s 396ms/step - loss: 0.0541 - accuracy: 0.9800\n",
      "Epoch 12/30\n",
      "417/417 [==============================] - 166s 397ms/step - loss: 0.0381 - accuracy: 0.9869\n",
      "Epoch 13/30\n",
      "417/417 [==============================] - 166s 398ms/step - loss: 0.0303 - accuracy: 0.9893\n",
      "Epoch 14/30\n",
      "417/417 [==============================] - 165s 397ms/step - loss: 0.0288 - accuracy: 0.9894\n",
      "Epoch 15/30\n",
      "417/417 [==============================] - 165s 396ms/step - loss: 0.0270 - accuracy: 0.9911\n",
      "Epoch 16/30\n",
      "417/417 [==============================] - 166s 398ms/step - loss: 0.0282 - accuracy: 0.9908\n",
      "Epoch 17/30\n",
      "417/417 [==============================] - 165s 396ms/step - loss: 0.0264 - accuracy: 0.9902\n",
      "Epoch 18/30\n",
      "417/417 [==============================] - 166s 398ms/step - loss: 0.0190 - accuracy: 0.9937\n",
      "Epoch 19/30\n",
      "417/417 [==============================] - 166s 397ms/step - loss: 0.0172 - accuracy: 0.9945\n",
      "Epoch 20/30\n",
      "417/417 [==============================] - 166s 398ms/step - loss: 0.0201 - accuracy: 0.9932\n",
      "Epoch 21/30\n",
      "417/417 [==============================] - 167s 399ms/step - loss: 0.0199 - accuracy: 0.9931\n",
      "Epoch 22/30\n",
      "417/417 [==============================] - 165s 397ms/step - loss: 0.0230 - accuracy: 0.9916\n",
      "Epoch 23/30\n",
      "417/417 [==============================] - 166s 398ms/step - loss: 0.0181 - accuracy: 0.9941\n",
      "Epoch 24/30\n",
      "417/417 [==============================] - 167s 400ms/step - loss: 0.0225 - accuracy: 0.9916\n",
      "Epoch 25/30\n",
      "417/417 [==============================] - 166s 399ms/step - loss: 0.0158 - accuracy: 0.9948\n",
      "Epoch 26/30\n",
      "417/417 [==============================] - 170s 409ms/step - loss: 0.0150 - accuracy: 0.9952\n",
      "Epoch 27/30\n",
      "417/417 [==============================] - 170s 407ms/step - loss: 0.0159 - accuracy: 0.9941\n",
      "Epoch 28/30\n",
      "417/417 [==============================] - 167s 400ms/step - loss: 0.0167 - accuracy: 0.9944\n",
      "Epoch 29/30\n",
      "417/417 [==============================] - 168s 402ms/step - loss: 0.0171 - accuracy: 0.9943\n",
      "Epoch 30/30\n",
      "417/417 [==============================] - 167s 401ms/step - loss: 0.0146 - accuracy: 0.9943\n",
      "209/209 [==============================] - 16s 77ms/step - loss: 0.8725 - accuracy: 0.8143\n",
      "Epoch 1/30\n",
      "417/417 [==============================] - 165s 395ms/step - loss: 0.8166 - accuracy: 0.6743\n",
      "Epoch 2/30\n",
      "417/417 [==============================] - 165s 395ms/step - loss: 0.5232 - accuracy: 0.7772\n",
      "Epoch 3/30\n",
      "417/417 [==============================] - 165s 396ms/step - loss: 0.4119 - accuracy: 0.8214\n",
      "Epoch 4/30\n",
      "417/417 [==============================] - 165s 397ms/step - loss: 0.3093 - accuracy: 0.8700\n",
      "Epoch 5/30\n",
      "417/417 [==============================] - 165s 396ms/step - loss: 0.2310 - accuracy: 0.9068\n",
      "Epoch 6/30\n",
      "417/417 [==============================] - 165s 396ms/step - loss: 0.1592 - accuracy: 0.9369\n",
      "Epoch 7/30\n",
      "417/417 [==============================] - 165s 396ms/step - loss: 0.1056 - accuracy: 0.9632\n",
      "Epoch 8/30\n",
      "417/417 [==============================] - 165s 397ms/step - loss: 0.0788 - accuracy: 0.9725\n",
      "Epoch 9/30\n",
      "417/417 [==============================] - 165s 395ms/step - loss: 0.0598 - accuracy: 0.9792\n",
      "Epoch 10/30\n",
      "417/417 [==============================] - 165s 397ms/step - loss: 0.0420 - accuracy: 0.9865\n",
      "Epoch 11/30\n",
      "417/417 [==============================] - 165s 395ms/step - loss: 0.0365 - accuracy: 0.9885\n",
      "Epoch 12/30\n",
      "417/417 [==============================] - 166s 397ms/step - loss: 0.0357 - accuracy: 0.9890\n",
      "Epoch 13/30\n",
      "417/417 [==============================] - 165s 395ms/step - loss: 0.0335 - accuracy: 0.9882\n",
      "Epoch 14/30\n",
      "417/417 [==============================] - 165s 395ms/step - loss: 0.0311 - accuracy: 0.9894\n",
      "Epoch 15/30\n",
      "417/417 [==============================] - 167s 400ms/step - loss: 0.0262 - accuracy: 0.9909\n",
      "Epoch 16/30\n",
      "417/417 [==============================] - 165s 395ms/step - loss: 0.0230 - accuracy: 0.9923\n",
      "Epoch 17/30\n",
      "417/417 [==============================] - 165s 396ms/step - loss: 0.0268 - accuracy: 0.9906\n",
      "Epoch 18/30\n",
      "417/417 [==============================] - 167s 400ms/step - loss: 0.0269 - accuracy: 0.9906\n",
      "Epoch 19/30\n",
      "417/417 [==============================] - 169s 406ms/step - loss: 0.0185 - accuracy: 0.9939\n",
      "Epoch 20/30\n",
      "417/417 [==============================] - 165s 396ms/step - loss: 0.0173 - accuracy: 0.9934\n",
      "Epoch 21/30\n",
      "417/417 [==============================] - 165s 396ms/step - loss: 0.0184 - accuracy: 0.9933\n",
      "Epoch 22/30\n",
      "417/417 [==============================] - 166s 398ms/step - loss: 0.0146 - accuracy: 0.9951\n",
      "Epoch 23/30\n",
      "417/417 [==============================] - 165s 395ms/step - loss: 0.0146 - accuracy: 0.9959\n",
      "Epoch 24/30\n",
      "417/417 [==============================] - 165s 396ms/step - loss: 0.0266 - accuracy: 0.9910\n",
      "Epoch 25/30\n",
      "417/417 [==============================] - 165s 395ms/step - loss: 0.0130 - accuracy: 0.9960\n",
      "Epoch 26/30\n",
      "417/417 [==============================] - 166s 399ms/step - loss: 0.0116 - accuracy: 0.9962\n",
      "Epoch 27/30\n",
      "417/417 [==============================] - 165s 395ms/step - loss: 0.0111 - accuracy: 0.9963\n",
      "Epoch 28/30\n",
      "417/417 [==============================] - 166s 398ms/step - loss: 0.0103 - accuracy: 0.9966\n",
      "Epoch 29/30\n",
      "417/417 [==============================] - 165s 397ms/step - loss: 0.0135 - accuracy: 0.9961\n",
      "Epoch 30/30\n",
      "417/417 [==============================] - 165s 395ms/step - loss: 0.0106 - accuracy: 0.9960\n",
      "209/209 [==============================] - 17s 80ms/step - loss: 0.8067 - accuracy: 0.8219\n",
      "Epoch 1/30\n",
      "625/625 [==============================] - 250s 401ms/step - loss: 0.7516 - accuracy: 0.6874\n",
      "Epoch 2/30\n",
      "625/625 [==============================] - 248s 397ms/step - loss: 0.4832 - accuracy: 0.7839\n",
      "Epoch 3/30\n",
      "625/625 [==============================] - 249s 398ms/step - loss: 0.3782 - accuracy: 0.8331\n",
      "Epoch 4/30\n",
      "625/625 [==============================] - 249s 398ms/step - loss: 0.2987 - accuracy: 0.8751\n",
      "Epoch 5/30\n",
      "625/625 [==============================] - 249s 398ms/step - loss: 0.2180 - accuracy: 0.9122\n",
      "Epoch 6/30\n",
      "625/625 [==============================] - 248s 397ms/step - loss: 0.1547 - accuracy: 0.9391\n",
      "Epoch 7/30\n",
      "625/625 [==============================] - 249s 398ms/step - loss: 0.1045 - accuracy: 0.9599\n",
      "Epoch 8/30\n",
      "625/625 [==============================] - 252s 403ms/step - loss: 0.0817 - accuracy: 0.9705\n",
      "Epoch 9/30\n",
      "625/625 [==============================] - 248s 397ms/step - loss: 0.0678 - accuracy: 0.9761\n",
      "Epoch 10/30\n",
      "625/625 [==============================] - 249s 399ms/step - loss: 0.0521 - accuracy: 0.9814\n",
      "Epoch 11/30\n",
      "625/625 [==============================] - 248s 397ms/step - loss: 0.0408 - accuracy: 0.9854\n",
      "Epoch 12/30\n",
      "625/625 [==============================] - 251s 401ms/step - loss: 0.0454 - accuracy: 0.9835\n",
      "Epoch 13/30\n",
      "625/625 [==============================] - 249s 398ms/step - loss: 0.0387 - accuracy: 0.9850\n",
      "Epoch 14/30\n",
      "625/625 [==============================] - 249s 398ms/step - loss: 0.0441 - accuracy: 0.9843\n",
      "Epoch 15/30\n",
      "625/625 [==============================] - 248s 397ms/step - loss: 0.0312 - accuracy: 0.9890\n",
      "Epoch 16/30\n",
      "625/625 [==============================] - 249s 399ms/step - loss: 0.0237 - accuracy: 0.9918\n",
      "Epoch 17/30\n",
      "625/625 [==============================] - 248s 396ms/step - loss: 0.0223 - accuracy: 0.9925\n",
      "Epoch 18/30\n",
      "625/625 [==============================] - 248s 397ms/step - loss: 0.0245 - accuracy: 0.9919\n",
      "Epoch 19/30\n",
      "625/625 [==============================] - 248s 397ms/step - loss: 0.0228 - accuracy: 0.9918\n",
      "Epoch 20/30\n",
      "625/625 [==============================] - 249s 398ms/step - loss: 0.0234 - accuracy: 0.9907\n",
      "Epoch 21/30\n",
      "625/625 [==============================] - 248s 397ms/step - loss: 0.0185 - accuracy: 0.9933\n",
      "Epoch 22/30\n",
      "625/625 [==============================] - 248s 397ms/step - loss: 0.0211 - accuracy: 0.9918\n",
      "Epoch 23/30\n",
      "625/625 [==============================] - 249s 398ms/step - loss: 0.0213 - accuracy: 0.9922\n",
      "Epoch 24/30\n",
      "625/625 [==============================] - 253s 405ms/step - loss: 0.0204 - accuracy: 0.9924\n",
      "Epoch 25/30\n",
      "625/625 [==============================] - 248s 396ms/step - loss: 0.0168 - accuracy: 0.9940\n",
      "Epoch 26/30\n",
      "625/625 [==============================] - 249s 399ms/step - loss: 0.0172 - accuracy: 0.9948\n",
      "Epoch 27/30\n",
      "625/625 [==============================] - 248s 397ms/step - loss: 0.0187 - accuracy: 0.9934\n",
      "Epoch 28/30\n",
      "625/625 [==============================] - 249s 398ms/step - loss: 0.0133 - accuracy: 0.9956\n",
      "Epoch 29/30\n",
      "625/625 [==============================] - 248s 396ms/step - loss: 0.0142 - accuracy: 0.9954\n",
      "Epoch 30/30\n",
      "625/625 [==============================] - 248s 397ms/step - loss: 0.0142 - accuracy: 0.9944\n"
     ]
    }
   ],
   "source": [
    "# define the grid search parameters\n",
    "batches     = [32]\n",
    "epochs      = [30]\n",
    "\n",
    "def create_model(learn_rate=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    optimizer = Adamax(learning_rate=learn_rate)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "param_grid = dict(batch_size=batches, epochs=epochs)\n",
    "\n",
    "tensorboard = create_tensorboard_callback()\n",
    "# create_checkpoint(model)\n",
    "# history = LossHistory()\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best: 0.816400 using {'batch_size': 32, 'epochs': 30}\n0.816400 (0.003950) with: {'batch_size': 32, 'epochs': 30}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best: 0.816400 using {'batch_size': 32, 'epochs': 30}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([5026.95805534]),\n",
       " 'std_fit_time': array([40.39280745]),\n",
       " 'mean_score_time': array([16.87704142]),\n",
       " 'std_score_time': array([0.24527117]),\n",
       " 'param_batch_size': masked_array(data=[32],\n",
       "              mask=[False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_epochs': masked_array(data=[30],\n",
       "              mask=[False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'batch_size': 32, 'epochs': 30}],\n",
       " 'split0_test_score': array([0.81295937]),\n",
       " 'split1_test_score': array([0.8143093]),\n",
       " 'split2_test_score': array([0.8219322]),\n",
       " 'mean_test_score': array([0.81640029]),\n",
       " 'std_test_score': array([0.00395028]),\n",
       " 'rank_test_score': array([1], dtype=int32)}"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "grid_result.cv_results_"
   ]
  },
  {
   "source": [
    "# Checking the TensorBoard logs\n",
    "\n",
    "Now our model has been trained, we can make its performance visual by checking the TensorBoard logs.\n",
    "\n",
    "The TensorBoard magic function (%tensorboard) will access the logs directory we created earlier and viualize its contents.\n",
    "\n",
    "\n",
    "Thanks to our early_stopping callback, the model stopped training after 26 or so epochs (in my case, yours might be slightly different). This is because the validation accuracy failed to improve for 3 epochs.\n",
    "\n",
    "But the good new is, we can definitely see our model is learning something. The validation accuracy got to 65% in only a few minutes.\n",
    "\n",
    "This means, if we were to scale up the number of images, hopefully we'd see the accuracy increase.\n",
    "\n",
    "To see the logs visit : http://localhost:6006 in your browser"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "
     },
     "metadata": {}
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "source": [
    "## Load weigts from checkpoint"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model(learn_rate=0.001):\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#     model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#     model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(512, activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "#     optimizer = Adamax(learning_rate=learn_rate)\n",
    "\n",
    "#     # Compile model\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training_2/cp-0002.ckpt\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'KerasClassifier' object has no attribute 'load_weights'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-4a7e8f28a555>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Loads the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'KerasClassifier' object has no attribute 'load_weights'"
     ]
    }
   ],
   "source": [
    "# model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "# model = create_model()\n",
    "\n",
    "# #create_checkpoint(model)\n",
    "# checkpoint_path = \"training_2/cp-{epoch:04d}.ckpt\"\n",
    "# checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "# print(latest)\n",
    "\n",
    "# # Loads the weights\n",
    "# model.load_weights(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('3.8.3': pyenv)",
   "language": "python",
   "name": "python38364bit383pyenv0293f2baa8be41b0b4bb6fbe3a94cc5f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}